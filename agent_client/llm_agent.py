import os, re, json, requests, time, argparse
from openai import OpenAI
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict
from agent_client.reviewer_agent import review_answer

from config.setting import OPENAI_API_KEY, JSON_CACHE, UNIFIED_SERVER_URL
# (1) ===== æ›´æ–° import èªå¥ =====
from config.prompts import (
    SYSTEM_PROMPT, DECOMPOSER_SYSTEM_PROMPT, USER_PROMPT_TEMPLATE,
    load_intents, build_llm_intent_doc
)

load_dotenv()
INTENTS = load_intents()
LLM_INTENT_DOC = build_llm_intent_doc(INTENTS)

load_dotenv()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ èªæ„æ‹†è§£èˆ‡è‡ªå‹• tool_call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_json(text: str) -> str:
    """
    å¾ LLM å›è¦†ä¸­æ“·å– JSON å€å¡Šï¼Œè‹¥ç„¡å‰‡å›å‚³åŸæ–‡
    """
    match = re.search(r"```(?:json)?\s*({[\s\S]*?})\s*```", text)
    if match:
        return match.group(1)
    return text

def parse_intent(user_query: str, intents=INTENTS):
    user_query_lc = user_query.lower()
    for intent in intents:
        if any(kw in user_query_lc for kw in intent.get("keywords", [])):
            return intent
    for intent in intents:
        if intent.get("intent") == "other":
            return intent
    return None

def decompose_query(user_input: str) -> dict:
    """
    LLM æ‹†è§£èªæ„ï¼Œä¾ DECOMPOSER_SYSTEM_PROMPT/USER_PROMPT_TEMPLATE è¦æ±‚ï¼Œç”¢ç”Ÿ intent/flags/tool_calls çµæ§‹
    """
    user_prompt = USER_PROMPT_TEMPLATE.format(user_input=user_input)
    messages = [
        # æ˜ç¢ºä½¿ç”¨ DECOMPOSER_SYSTEM_PROMPT
        {"role": "system", "content": DECOMPOSER_SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt}
    ]
    response_text = call_llm(messages)
    json_part = extract_json(response_text)
    try:
        result = json.loads(json_part)
    except json.JSONDecodeError:
        raise ValueError("âŒ ç„¡æ³•è§£æ LLM å›æ‡‰ï¼š", response_text)
    return result

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# çµ±ä¸€å‘¼å« unified_server æŸ¥è©¢
def call_server(tool, args, retry=2):
    for attempt in range(retry):
        try:
            resp = requests.get(UNIFIED_SERVER_URL, params={"type": tool, **args}, timeout=10)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            if attempt < retry-1:
                time.sleep(1)
                continue
            else:
                return {"status": "ERROR", "data": str(e)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å½ˆæ€§æ‘˜è¦å„ç¨® tool çš„å›å‚³çµæœï¼ˆå…¨ tool æ”¯æ´ï¼‰
def summarize_tool_result(tool, tool_result):
    status = tool_result.get("status", "").lower()
    data = tool_result.get("data", [])

    # === ç•°å¸¸ç¸½è¦½ï¼šæ¢åˆ—åŒ–æ¯å€‹æ‰¹æ¬¡ ===
    if tool == "batch_anomaly":
        # print("[DEBUG] batch_anomaly data:", json.dumps(data, ensure_ascii=False, indent=2))
        if not data:
            return "ç„¡ç•°å¸¸æ‰¹æ¬¡"
        abnormal_batches = []
        for item in data:
            if item.get("abnormal_count", 0) > 0 or item.get("abnormal_features"):
                lines = [
                    f"- æ‰¹æ¬¡IDï¼š{item.get('batch_id', '')}",
                    f"- ç”¢å“ï¼š{item.get('product', item.get('product_name', ''))}",
                    f"- ç•°å¸¸æ•¸ï¼š{item.get('abnormal_count', '')}",
                    f"- ç•°å¸¸ä¸»å› ï¼š{item.get('main_reason', '') or item.get('anomaly_remark', '') or 'æœªæ¨™è¨»'}",
                ]
                # æ¢åˆ—ç‰¹æ€§
                for f in item.get("abnormal_features", []):
                    feature_lines = [f"  - ç‰¹æ€§ï¼š{f.get('feature_name', '')}"]
                    if f.get('cpk_alert'):
                        feature_lines.append(f"    - Cpk={f.get('cpk', '')} [è­¦å‘Š:{f.get('cpk_reason', '')}]")
                    if f.get('ppk_alert'):
                        feature_lines.append(f"    - Ppk={f.get('ppk', '')} [è­¦å‘Š:{f.get('ppk_reason', '')}]")
                    if f.get("abnormal_detail"):
                        feature_lines.append(f"    - æ˜ç´°: {', '.join(map(str, f['abnormal_detail']))}")
                    lines.extend(feature_lines)
                abnormal_batches.append("\n".join(lines))
        if not abnormal_batches:
            return "æŸ¥ç„¡ç•°å¸¸æ‰¹æ¬¡"
        return "\n\n".join(abnormal_batches)

    # === ç•°å¸¸è¶¨å‹¢ï¼šæ¢åˆ—åŒ–æ¯ç­†ç•°å¸¸ ===
    if tool == "anomaly_trend":
        if not data:
            return "æœ¬å€é–“ç„¡ç•°å¸¸è¶¨å‹¢"
        lines = []
        for item in data:
            count = item.get('count', 0)
            try:
                count_num = int(count)
            except Exception:
                count_num = 0
            if count_num > 0:
                lines.append(
                    f"- æ—¥æœŸï¼š{item.get('date', '')}\n"
                    f"  - æ©Ÿå°ï¼š{item.get('machine_id', '')}\n"
                    f"  - ç”¢ç·šï¼š{item.get('line', '')}\n"
                    f"  - ç•°å¸¸é¡å‹ï¼š{item.get('event_type', '')}\n"
                    f"  - ç•°å¸¸ä»£ç¢¼ï¼š{item.get('abnormal_code', '')}\n"
                    f"  - æ¬¡æ•¸ï¼š{count}\n"
                    f"  - å‚™è¨»ï¼š{item.get('anomaly_remark', '')}"
                )
        if not lines:
            return "æœ¬å€é–“æœªç™¼ç¾ç•°å¸¸è¶¨å‹¢"
        return "\n\n".join(lines)

    # å…¶ä»– summary ä¿æŒåŸæœ¬æ ¼å¼ï¼ˆè¡¨æ ¼æˆ–ç´”æ–‡å­—ï¼‰
    if tool == "spc_summary":
        if not data:
            return "ç„¡ SPC è£½ç¨‹èƒ½åŠ›è³‡æ–™"
        spc_lines = []
        for item in data:
            for f in item.get("spc_items", []):
                if f.get("cpk_alert") or f.get("ppk_alert"):
                    spc_lines.append(
                        f"æ‰¹æ¬¡:{item.get('batch_id','')}, ç‰¹æ€§:{f.get('feature_name','')}, Cpk:{f.get('cpk','')}, Ppk:{f.get('ppk','')}, è­¦å‘Š:{f.get('cpk_reason','') or f.get('ppk_reason','')}"
                    )
        if not spc_lines:
            return "æ‰€æœ‰æ‰¹æ¬¡SPCçš†æ­£å¸¸"
        return "\n".join(spc_lines)

    if tool == "production_summary":
        if not data:
            return "æœ¬æ—¥ç„¡ç”¢èƒ½è³‡æ–™"
        header = ["æ©Ÿå°", "ç”¢ç·š", "ç­åˆ¥", "ç›®æ¨™ç”¢é‡", "å¯¦éš›ç”¢é‡", "é”æˆç‡(%)"]
        rows = [
            [row.get("machine_id", ""), row.get("line", ""), row.get("shift", ""),
             row.get("target_qty", ""), row.get("actual_qty", ""), row.get("achieve_rate", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "downtime_summary":
        if not data:
            return "æœ¬æ—¥ç„¡åœæ©Ÿç´€éŒ„"
        header = ["æ©Ÿå°", "ç”¢ç·š", "ç­åˆ¥", "åœæ©Ÿæ¬¡æ•¸", "åœæ©Ÿç¸½æ™‚æ•¸(åˆ†é˜)", "ä¸»å› ", "å‚™è¨»"]
        rows = [
            [row.get("machine_id", ""), row.get("line", ""), row.get("shift", ""), row.get("event_count", ""),
             row.get("total_minutes", ""), row.get("main_reason", ""), row.get("remark", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "yield_summary":
        if not data:
            return "æœ¬æ—¥ç„¡è‰¯ç‡ç´€éŒ„"
        header = ["ç”¢å“", "ç”¢ç·š", "ç­åˆ¥", "è‰¯å“æ•¸", "ä¸è‰¯å“æ•¸", "è‰¯ç‡(%)"]
        rows = [
            [row.get("product", ""), row.get("line", ""), row.get("shift", ""),
             row.get("good_qty", ""), row.get("ng_qty", ""), row.get("yield_percent", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "anomaly_trend":
        if not data:
            return "æœ¬å€é–“ç„¡ç•°å¸¸è¶¨å‹¢"
        lines = []
        header = "æ—¥æœŸ\tæ©Ÿå°\tç”¢ç·š\tç•°å¸¸é¡å‹\tç•°å¸¸ä»£ç¢¼\tæ¬¡æ•¸\tå‚™è¨»"
        lines.append(header)
        show = False
        for item in data:
            # åªé¡¯ç¤ºæœ‰ç•°å¸¸(æ¬¡æ•¸>0)çš„ç´€éŒ„
            count = item.get('count', 0)
            # éƒ¨åˆ†è³‡æ–™å‹æ…‹å¯èƒ½æ˜¯å­—ä¸²ï¼Œè¦è½‰æˆæ•¸å­—
            try:
                count_num = int(count)
            except Exception:
                count_num = 0
            if count_num > 0:
                show = True
                row = "\t".join([
                    str(item.get('date', '')),
                    str(item.get('machine_id', '')),
                    str(item.get('line', '')),
                    str(item.get('event_type', '')),
                    str(item.get('abnormal_code', '')),
                    str(count),
                    str(item.get('anomaly_remark', ''))
                ])
                lines.append(row)
        if not show:
            return "æœ¬å€é–“æœªç™¼ç¾ç•°å¸¸è¶¨å‹¢"
        return "\n".join(lines)

    if tool == "KPI_summary":
        if not data:
            return "æœ¬æ—¥ç„¡KPIç´€éŒ„"
        header = list(data[0].keys())
        table = "\t".join(header) + "\n"
        for row in data:
            table += "\t".join([str(row.get(h, "")) for h in header]) + "\n"
        return table

    if tool == "issue_tracker":
        if not data:
            return "ç„¡æœªçµæ¡ˆå·¥å–®"
        header = list(data[0].keys())
        table = "\t".join(header) + "\n"
        for row in data:
            table += "\t".join([str(row.get(h, "")) for h in header]) + "\n"
        return table

    if status != "ok":
        return f"ã€ç„¡è³‡æ–™/ç•°å¸¸: {tool_result.get('data','')}ã€‘"
    return json.dumps(data, ensure_ascii=False, indent=2) if data else "ç„¡è³‡æ–™"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å½™ç¸½æ‰¹æ¬¡çš„å„å·¥å…·æ‘˜è¦
def summarize_batch_context(batch_id, tool_results: Dict[str, str]):
    # åªæ“·å–ç•°å¸¸æ•¸èˆ‡SPCè­¦å‘Šæ•¸ç­‰é‡é»
    batch_anomaly = tool_results.get("batch_anomaly", "")
    spc_summary = tool_results.get("spc_summary", "")

    # æ“·å–ç•°å¸¸æ•¸
    abnormal_count = 0
    if "ç•°å¸¸æ•¸ï¼š" in batch_anomaly:
        try:
            abnormal_count = int(batch_anomaly.split("ç•°å¸¸æ•¸ï¼š")[1].split("\n")[0])
        except Exception:
            abnormal_count = 0

    # æ“·å–SPCè­¦å‘Šæ•¸
    spc_alert_count = 0
    if "è­¦å‘Š: æ˜¯" in spc_summary:
        spc_alert_count = spc_summary.count("è­¦å‘Š: æ˜¯")

    summary = (
        f"æ‰¹æ¬¡ï¼š{batch_id} | ç•°å¸¸æ•¸ï¼š{abnormal_count} | SPCè­¦å‘Šæ•¸ï¼š{spc_alert_count}\n"
    )
    return summary

# === 1. å¤šæ­¥æŸ¥è©¢ + å®¹éŒ¯ + ä¸Šä¸‹æ–‡ + LLMåŒæ­¥intents ===
def run_agent_smart(user_query, session_history=None, return_summary=False, max_steps=3):
    # é€™å€‹ session_history ç¾åœ¨æœƒç”± ui.py æ­£ç¢ºå‚³å…¥
    history = session_history or []
    step_outputs = ["", "", "", "", "", ""]

    # --- æ­¥é©Ÿ 1: èªæ„åˆ†æ (åˆ¤æ–·æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·) ---
    decomp_result = None
    try:
        decomp_result = decompose_query(user_query)
        step_outputs[0] = json.dumps(decomp_result, ensure_ascii=False, indent=2)
    except Exception as e:
        step_outputs[0] = f"èªæ„åˆ†æå¤±æ•—: {e}"
    yield 0, step_outputs[0]

    # --- æ­¥é©Ÿ 2: æª¢æŸ¥æ˜¯å¦éœ€è¦å‘¼å«å·¥å…· ---
    tool_calls = decomp_result.get("tool_calls", []) if decomp_result else []
    if not tool_calls and decomp_result and decomp_result.get("flags"):
        tool_calls = [{"tool": flag, "args": {}} for flag in decomp_result["flags"]]
    
    # ===== æ–°å¢çš„åˆ¤æ–·é‚è¼¯ =====
    # å¦‚æœèªæ„åˆ†æå¾Œæ²’æœ‰ç”¢ç”Ÿä»»ä½• tool_callsï¼Œä»£è¡¨é€™æ˜¯ä¸€å€‹ç´”èŠå¤©/è¿½å•
    if not tool_calls:
        step_outputs[1] = "(ä¸éœ€å‘¼å«å·¥å…·ï¼Œé€²å…¥èŠå¤©æ¨¡å¼)"
        yield 1, step_outputs[1]
        step_outputs[2] = "(è·³é)"
        yield 2, step_outputs[2]
        step_outputs[3] = "(è·³é)"
        yield 3, step_outputs[3]
        step_outputs[4] = "(è·³é)"
        yield 4, step_outputs[4]

        messages_for_followup = [
            {"role": "system", "content": SYSTEM_PROMPT},
            *history 
        ]
        
        llm_reply = call_llm(messages_for_followup)
        step_outputs[5] = llm_reply
        yield 5, step_outputs[5]
        yield "done", (step_outputs, llm_reply, "") # èŠå¤©æ¨¡å¼æ²’æœ‰æ–°çš„æ‘˜è¦
        return # æå‰çµæŸå‡½å¼

    # ===== å¦‚æœéœ€è¦å‘¼å«å·¥å…·ï¼Œå‰‡åŸ·è¡ŒåŸæœ‰æµç¨‹ =====
    step_outputs[1] = json.dumps(tool_calls, ensure_ascii=False, indent=2)
    yield 1, step_outputs[1]
    
    # --- æ­¥é©Ÿ 3: agent tool_call ---
    tool_call_strs = [f"tool: {call.get('tool', '')}, args: {call.get('args', {})}" for call in tool_calls]
    step_outputs[2] = "\n".join(tool_call_strs)
    yield 2, step_outputs[2]

    # --- æ­¥é©Ÿ 4: serverå›å‚³ ---
    tool_results_dict = {}
    tool_result_summaries = []
    for call in tool_calls:
        tool = call.get("tool", "")
        args = call.get("args", {}) or {}
        tool_result = call_server(tool, args, retry=2)
        tool_results_dict[tool] = tool_result
        summary_str = summarize_tool_result(tool, tool_result)
        tool_result_summaries.append(f"ã€{tool}ã€‘\n{summary_str}")
    step_outputs[3] = "\n---\n".join(tool_result_summaries) if tool_result_summaries else "(ç„¡serverå›å‚³)"
    yield 3, step_outputs[3]
    
    # --- æ­¥é©Ÿ 5: çµ„è£æ‘˜è¦ ---
    summary_list = [f"ã€{tool} æ‘˜è¦ã€‘\n{summarize_tool_result(tool, result)}\n" for tool, result in tool_results_dict.items()]
    summary_section = "\n".join(summary_list)
    step_outputs[4] = summary_section if summary_section else "(ç„¡æ‘˜è¦)"
    yield 4, step_outputs[4]

    # --- æ­¥é©Ÿ 6: LLMå›è¦† ---
    final_user_prompt = (
        f"é€™æ˜¯æœ¬æ¬¡æŸ¥è©¢çš„ç›¸é—œè³‡æ–™ï¼š\n"
        f"---------------------\n"
        f"ã€å·¥å…·æŸ¥è©¢çµæœã€‘\n{summary_section}\n"
        f"---------------------\n\n"
        f"ç¾åœ¨ï¼Œè«‹æ ¹æ“šä»¥ä¸Šè³‡æ–™ï¼Œä¸¦åš´æ ¼éµå®ˆä½ çš„ç³»çµ±æŒ‡ä»¤ï¼Œå›ç­”æˆ‘æœ€åˆçš„å•é¡Œï¼š\n"
        f"ã€Œ{user_query}ã€"
    )

    messages_final = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": final_user_prompt}
    ]

    llm_reply = call_llm(messages_final)
    
    # --- é›™å‘å›æˆ (Reviewer Agent) æµç¨‹ ---
    reviewer_result = review_answer(user_query, summary_section, llm_reply)
    if not reviewer_result.get("answer_ok", True):
        # é€™è£¡å¯ä»¥åŠ å…¥ä¿®æ­£é‚è¼¯ï¼Œä½†ç‚ºç°¡åŒ–ï¼Œæˆ‘å€‘å…ˆå‡è¨­ç¬¬ä¸€è¼ªå›è¦†æ˜¯å¥½çš„
        pass

    step_outputs[5] = llm_reply
    yield 5, step_outputs[5]
    yield "done", (step_outputs, llm_reply, summary_section)

def run_agent(user_query, session_history=None, return_summary=False):
    result = run_agent_smart(user_query, session_history=session_history, return_summary=True)
    if return_summary:
        return result
    else:
        return result[1]  # åªå›å‚³ reply

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å‘¼å« LLM ç”Ÿæˆå»ºè­°
def call_llm(messages, model="gpt-3.5-turbo", api_key=None, temperature=0.0):
    api_key = api_key or OPENAI_API_KEY
    client = OpenAI(api_key=api_key)
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[LLMå‘¼å«å¤±æ•—] {str(e)}"

# ========= ä¸»è¦æµç¨‹ =========
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch", help="æ‰¹æ¬¡IDï¼Œå¤šå€‹ç”¨é€—è™Ÿåˆ†éš”ã€‚ä¸æŒ‡å®šå‰‡è‡ªå‹•æŠ“å…¨éƒ¨ã€‚")
    parser.add_argument("--extra", nargs="*", help="å…¶ä»–åƒæ•¸ï¼Œå¦‚ date=2024-05-28")
    parser.add_argument("--decompose", action="store_true", help="åƒ…é€²è¡Œèªæ„åˆ†é¡èˆ‡ tool_call æ‹†è§£")
    args = parser.parse_args()

    if args.decompose:
        query = input("è«‹è¼¸å…¥è¦åˆ†æçš„å•é¡Œï¼š")
        result = decompose_query(query)
        print("\nâœ… èªæ„åˆ†é¡ï¼š", result.get("intent"))
        print("ğŸ§© tool_calls æ‹†è§£ï¼š")
        for i, call in enumerate(result.get("tool_calls", []), 1):
            print(f"  {i}. {call}")
        exit(0)

    # å–å¾—æ‰€æœ‰æ‰¹æ¬¡ID
    if args.batch:
        batch_ids = [x.strip().zfill(2) for x in args.batch.split(",") if x.strip()]
    else:
        cache_dir = Path(JSON_CACHE)
        batch_ids = [f.stem for f in cache_dir.glob("*.json")]
        batch_ids.sort()

    # è™•ç†é¡å¤–åƒæ•¸
    extra_dict = {}
    if args.extra:
        for item in args.extra:
            if "=" in item:
                k, v = item.split("=", 1)
                extra_dict[k] = v

    # è®“ä½¿ç”¨è€…è¼¸å…¥è‡ªç„¶èªè¨€æŸ¥è©¢
    query = input("è«‹è¼¸å…¥æŸ¥è©¢éœ€æ±‚ï¼š")
    decomp = decompose_query(query)
    tool_calls = decomp.get("tool_calls", [])
    intent = decomp.get("intent", "")

    all_batch_summaries = []
    for batch_id in batch_ids:
        user_input_dict = {"batch_id": batch_id}
        user_input_dict.update(extra_dict)
        tool_results = {}
        for call in tool_calls:
            tool = call["tool"]
            args_dict = {arg: user_input_dict.get(arg, "") for arg in call["args"]}
            tool_result = call_server(tool, args_dict)
            tool_results[tool] = summarize_tool_result(tool, tool_result)
        all_batch_summaries.append(summarize_batch_context(batch_id, tool_results))

    # çµ„åˆæ‰€æœ‰æ‰¹æ¬¡çš„é‡é»æ‘˜è¦
    prompt = "ä¸‹æ–¹ç‚ºå¤šå€‹æ‰¹æ¬¡çš„é‡é»æ‘˜è¦ï¼š\n"
    for batch_summary in all_batch_summaries:
        prompt += batch_summary
    prompt += "\nè«‹ä¾æ“šä¸Šè¿°æ‰€æœ‰æ‰¹æ¬¡è³‡è¨Šï¼Œæ­¸ç´å„æ‰¹æ¬¡çš„å“è³ªå•é¡Œã€æ˜¯å¦éœ€ç¾å ´æ”¹å–„ï¼Œä¸¦æå‡ºç¸½é«”æª¢è¨/æ”¹å–„å»ºè­°ã€‚"

    system_msg = "ä½ æ˜¯ç”¢ç·šå°ˆå®¶ï¼Œè«‹æ ¹æ“šè³‡æ–™ç”¢ç”Ÿå°ˆæ¥­å»ºè­°ã€‚"
    print("=== å‚³é€çµ¦ LLM çš„ prompt ===\n", prompt)
    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": prompt}
    ]
    llm_reply = call_llm(messages)
    print("=== LLM å›è¦† ===\n", llm_reply)

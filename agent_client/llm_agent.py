import os, re, json, requests, time, argparse
from openai import OpenAI
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict

from config.setting import OPENAI_API_KEY, JSON_CACHE, UNIFIED_SERVER_URL
from config.prompts import (
    SYSTEM_PROMPT, USER_PROMPT_TEMPLATE,
    load_intents, build_llm_intent_doc
)

load_dotenv()
INTENTS = load_intents()
LLM_INTENT_DOC = build_llm_intent_doc(INTENTS)

load_dotenv()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ èªæ„æ‹†è§£èˆ‡è‡ªå‹• tool_call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_json(text: str) -> str:
    """
    å¾ LLM å›è¦†ä¸­æ“·å– JSON å€å¡Šï¼Œè‹¥ç„¡å‰‡å›å‚³åŸæ–‡
    """
    match = re.search(r"```(?:json)?\s*({[\s\S]*?})\s*```", text)
    if match:
        return match.group(1)
    return text

def parse_intent(user_query: str, intents=INTENTS):
    user_query_lc = user_query.lower()
    for intent in intents:
        if any(kw in user_query_lc for kw in intent.get("keywords", [])):
            return intent
    for intent in intents:
        if intent.get("intent") == "other":
            return intent
    return None

def decompose_query(user_input: str) -> dict:
    """
    LLM æ‹†è§£èªæ„ï¼Œä¾ SYSTEM_PROMPT/USER_PROMPT_TEMPLATE è¦æ±‚ï¼Œç”¢ç”Ÿ intent/flags/tool_calls çµæ§‹
    """
    user_prompt = USER_PROMPT_TEMPLATE.format(user_input=user_input)
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt}
    ]
    response_text = call_llm(messages)
    json_part = extract_json(response_text)
    try:
        result = json.loads(json_part)
    except json.JSONDecodeError:
        raise ValueError("âŒ ç„¡æ³•è§£æ LLM å›æ‡‰ï¼š", response_text)
    return result

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# çµ±ä¸€å‘¼å« unified_server æŸ¥è©¢
def call_server(tool, args, retry=2):
    for attempt in range(retry):
        try:
            resp = requests.get(UNIFIED_SERVER_URL, params={"type": tool, **args}, timeout=10)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            if attempt < retry-1:
                time.sleep(1)
                continue
            else:
                return {"status": "ERROR", "data": str(e)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å½ˆæ€§æ‘˜è¦å„ç¨® tool çš„å›å‚³çµæœï¼ˆå…¨ tool æ”¯æ´ï¼‰
def summarize_tool_result(tool, tool_result):
    status = tool_result.get("status", "").lower()
    data = tool_result.get("data", [])

    # === ç•°å¸¸ç¸½è¦½ï¼šç›´æ¥å½™æ•´æ‰€æœ‰ç•°å¸¸æ‰¹æ¬¡ ===
    if tool == "batch_anomaly":
        # print("[DEBUG] batch_anomaly data:", json.dumps(data, ensure_ascii=False, indent=2))
        if not data:
            return "ç„¡ç•°å¸¸æ‰¹æ¬¡"
        abnormal_batches = []
        for item in data:
            # åªç¯©ç•°å¸¸æ‰¹æ¬¡
            if item.get("abnormal_count", 0) > 0 or item.get("abnormal_features"):
                lines = [
                    f"æ‰¹æ¬¡IDï¼š{item.get('batch_id', '')}",
                    f"ç”¢å“ï¼š{item.get('product', item.get('product_name', ''))}",
                    f"ç•°å¸¸æ•¸ï¼š{item.get('abnormal_count', '')}",
                    f"ç•°å¸¸ä¸»å› ï¼š{item.get('main_reason', '') or item.get('anomaly_remark', '') or 'æœªæ¨™è¨»'}",
                ]
                # æŒ‡æ¨™ç´°ç¯€
                for f in item.get("abnormal_features", []):
                    desc = f"- ç‰¹æ€§ï¼š{f.get('feature_name', '')}"
                    if f.get('cpk_alert'):
                        desc += f" | Cpk={f.get('cpk', '')} [è­¦å‘Š:{f.get('cpk_reason', '')}]"
                    if f.get('ppk_alert'):
                        desc += f" | Ppk={f.get('ppk', '')} [è­¦å‘Š:{f.get('ppk_reason', '')}]"
                    if f.get("abnormal_detail"):
                        desc += f" | æ˜ç´°: {','.join(map(str, f['abnormal_detail']))}"
                    lines.append(desc)
                abnormal_batches.append("\n".join(lines))
        if not abnormal_batches:
            return "æŸ¥ç„¡ç•°å¸¸æ‰¹æ¬¡"
        return "\n---\n".join(abnormal_batches)

    # =========== SPCæŒ‡æ¨™ =============
    if tool == "spc_summary":
        if not data:
            return "ç„¡ SPC è£½ç¨‹èƒ½åŠ›è³‡æ–™"
        spc_lines = []
        for item in data:
            for f in item.get("spc_items", []):
                if f.get("cpk_alert") or f.get("ppk_alert"):
                    spc_lines.append(
                        f"æ‰¹æ¬¡:{item.get('batch_id','')}, ç‰¹æ€§:{f.get('feature_name','')}, Cpk:{f.get('cpk','')}, Ppk:{f.get('ppk','')}, è­¦å‘Š:{f.get('cpk_reason','') or f.get('ppk_reason','')}"
                    )
        if not spc_lines:
            return "æ‰€æœ‰æ‰¹æ¬¡SPCçš†æ­£å¸¸"
        return "\n".join(spc_lines)

    if tool == "production_summary":
        if not data:
            return "æœ¬æ—¥ç„¡ç”¢èƒ½è³‡æ–™"
        header = ["æ©Ÿå°", "ç”¢ç·š", "ç­åˆ¥", "ç›®æ¨™ç”¢é‡", "å¯¦éš›ç”¢é‡", "é”æˆç‡(%)"]
        rows = [
            [row.get("machine_id", ""), row.get("line", ""), row.get("shift", ""),
             row.get("target_qty", ""), row.get("actual_qty", ""), row.get("achieve_rate", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "downtime_summary":
        if not data:
            return "æœ¬æ—¥ç„¡åœæ©Ÿç´€éŒ„"
        header = ["æ©Ÿå°", "ç”¢ç·š", "ç­åˆ¥", "åœæ©Ÿæ¬¡æ•¸", "åœæ©Ÿç¸½æ™‚æ•¸(åˆ†é˜)", "ä¸»å› ", "å‚™è¨»"]
        rows = [
            [row.get("machine_id", ""), row.get("line", ""), row.get("shift", ""), row.get("event_count", ""),
             row.get("total_minutes", ""), row.get("main_reason", ""), row.get("remark", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "yield_summary":
        if not data:
            return "æœ¬æ—¥ç„¡è‰¯ç‡ç´€éŒ„"
        header = ["ç”¢å“", "ç”¢ç·š", "ç­åˆ¥", "è‰¯å“æ•¸", "ä¸è‰¯å“æ•¸", "è‰¯ç‡(%)"]
        rows = [
            [row.get("product", ""), row.get("line", ""), row.get("shift", ""),
             row.get("good_qty", ""), row.get("ng_qty", ""), row.get("yield_percent", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "anomaly_trend":
        if not data:
            return "æœ¬å€é–“ç„¡ç•°å¸¸è¶¨å‹¢"
        lines = []
        header = "æ—¥æœŸ\tæ©Ÿå°\tç”¢ç·š\tç•°å¸¸é¡å‹\tç•°å¸¸ä»£ç¢¼\tæ¬¡æ•¸\tå‚™è¨»"
        lines.append(header)
        show = False
        for item in data:
            # åªé¡¯ç¤ºæœ‰ç•°å¸¸(æ¬¡æ•¸>0)çš„ç´€éŒ„
            count = item.get('count', 0)
            # éƒ¨åˆ†è³‡æ–™å‹æ…‹å¯èƒ½æ˜¯å­—ä¸²ï¼Œè¦è½‰æˆæ•¸å­—
            try:
                count_num = int(count)
            except Exception:
                count_num = 0
            if count_num > 0:
                show = True
                row = "\t".join([
                    str(item.get('date', '')),
                    str(item.get('machine_id', '')),
                    str(item.get('line', '')),
                    str(item.get('event_type', '')),
                    str(item.get('abnormal_code', '')),
                    str(count),
                    str(item.get('anomaly_remark', ''))
                ])
                lines.append(row)
        if not show:
            return "æœ¬å€é–“æœªç™¼ç¾ç•°å¸¸è¶¨å‹¢"
        return "\n".join(lines)

    if tool == "KPI_summary":
        if not data:
            return "æœ¬æ—¥ç„¡KPIç´€éŒ„"
        header = list(data[0].keys())
        table = "\t".join(header) + "\n"
        for row in data:
            table += "\t".join([str(row.get(h, "")) for h in header]) + "\n"
        return table

    if tool == "issue_tracker":
        if not data:
            return "ç„¡æœªçµæ¡ˆå·¥å–®"
        header = list(data[0].keys())
        table = "\t".join(header) + "\n"
        for row in data:
            table += "\t".join([str(row.get(h, "")) for h in header]) + "\n"
        return table

    if status != "ok":
        return f"ã€ç„¡è³‡æ–™/ç•°å¸¸: {tool_result.get('data','')}ã€‘"
    return json.dumps(data, ensure_ascii=False, indent=2) if data else "ç„¡è³‡æ–™"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å½™ç¸½æ‰¹æ¬¡çš„å„å·¥å…·æ‘˜è¦
def summarize_batch_context(batch_id, tool_results: Dict[str, str]):
    # åªæ“·å–ç•°å¸¸æ•¸èˆ‡SPCè­¦å‘Šæ•¸ç­‰é‡é»
    batch_anomaly = tool_results.get("batch_anomaly", "")
    spc_summary = tool_results.get("spc_summary", "")

    # æ“·å–ç•°å¸¸æ•¸
    abnormal_count = 0
    if "ç•°å¸¸æ•¸ï¼š" in batch_anomaly:
        try:
            abnormal_count = int(batch_anomaly.split("ç•°å¸¸æ•¸ï¼š")[1].split("\n")[0])
        except Exception:
            abnormal_count = 0

    # æ“·å–SPCè­¦å‘Šæ•¸
    spc_alert_count = 0
    if "è­¦å‘Š: æ˜¯" in spc_summary:
        spc_alert_count = spc_summary.count("è­¦å‘Š: æ˜¯")

    summary = (
        f"æ‰¹æ¬¡ï¼š{batch_id} | ç•°å¸¸æ•¸ï¼š{abnormal_count} | SPCè­¦å‘Šæ•¸ï¼š{spc_alert_count}\n"
    )
    return summary

# === 1. å¤šæ­¥æŸ¥è©¢ + å®¹éŒ¯ + ä¸Šä¸‹æ–‡ + LLMåŒæ­¥intents ===
def run_agent_smart(user_query, session_history=None, return_summary=False, max_steps=3):
    import json as _json
    context = []
    tool_results_dict = {}
    cur_query = user_query
    history = session_history or []
    step_outputs = ["", "", "", "", "", ""]

    # 1. èªæ„åˆ†æ
    decomp_result = None
    try:
        decomp_result = decompose_query(user_query)
        step_outputs[0] = _json.dumps(decomp_result, ensure_ascii=False, indent=2)
    except Exception as e:
        step_outputs[0] = f"èªæ„åˆ†æå¤±æ•—: {e}"

    # 2. å­å•é¡Œæ‹†è§£
    tool_calls = decomp_result.get("tool_calls", []) if decomp_result else []
    if not tool_calls and decomp_result and decomp_result.get("flags"):
        tool_calls = []
        for flag in decomp_result["flags"]:
            tool_calls.append({"tool": flag, "args": {}})
    step_outputs[1] = _json.dumps(tool_calls, ensure_ascii=False, indent=2) if tool_calls else "(ç„¡å­å•é¡Œæ‹†è§£)"

    # 3. agent tool_call
    tool_call_strs = []
    for call in tool_calls:
        tool = call.get("tool", "")
        args = call.get("args", {}) or {}
        tool_call_strs.append(f"tool: {tool}, args: {args}")
    step_outputs[2] = "\n".join(tool_call_strs) if tool_call_strs else "(ç„¡tool_call)"

    # 4. serverå›å‚³ï¼ˆè‡ªå‹•å®¹éŒ¯/è£œæŸ¥ï¼‰
    tool_results_this = {}
    tool_result_summaries = []
    for call in tool_calls:
        tool = call.get("tool", "")
        args = call.get("args", {}) or {}
        tool_result = call_server(tool, args, retry=2)
        # å®¹éŒ¯ï¼šè‹¥æŸ¥ç„¡è³‡æ–™ä¸”æœ‰å‚™ç”¨ toolï¼Œå¯è‡ªå‹•è£œæŸ¥
        if (not tool_result or tool_result.get("status", "").lower() != "ok") and tool in ["batch_anomaly", "spc_summary"]:
            # ä¾‹å¦‚ batch_anomaly æŸ¥ç„¡è³‡æ–™æ™‚è‡ªå‹•æŸ¥ anomaly_trend
            if tool == "batch_anomaly":
                fallback_result = call_server("anomaly_trend", args, retry=2)
                tool_results_this["anomaly_trend"] = fallback_result
                summary_str = summarize_tool_result("anomaly_trend", fallback_result)
                tool_result_summaries.append(f"ã€anomaly_trendã€‘\n{summary_str}")
            elif tool == "spc_summary":
                fallback_result = call_server("production_summary", args, retry=2)
                tool_results_this["production_summary"] = fallback_result
                summary_str = summarize_tool_result("production_summary", fallback_result)
                tool_result_summaries.append(f"ã€production_summaryã€‘\n{summary_str}")
        tool_results_this[tool] = tool_result
        summary_str = summarize_tool_result(tool, tool_result)
        tool_result_summaries.append(f"ã€{tool}ã€‘\n{summary_str}")
    tool_results_dict.update(tool_results_this)
    step_outputs[3] = "\n---\n".join(tool_result_summaries) if tool_result_summaries else "(ç„¡serverå›å‚³)"

    # 5. çµ„è£æ‘˜è¦ï¼ˆå½™æ•´æ‰€æœ‰å·¥å…·çµæœï¼Œä¸é‡è¤‡ï¼‰
    summary_list = []
    for tool, result in tool_results_dict.items():
        summary_str = summarize_tool_result(tool, result)
        summary_list.append(f"ã€{tool} æ‘˜è¦ã€‘\n{summary_str}\n")
    current_summary = "\n".join(summary_list)
    step_outputs[4] = current_summary if current_summary else "(ç„¡æ‘˜è¦)"

    # 6. LLMå›è¦†
    user_intent = user_query.strip()
    # sys_prompt èˆ‡ combined_summary éœ€æå‰å®šç¾©
    sys_prompt = (
        "ä½ æ˜¯æ™ºæ…§ç”¢ç·šæ™ºæ…§åŠ©ç†ï¼Œèƒ½è‡ªå‹•ä¸²æ¥å¤šå€‹MCPå·¥å…·ï¼Œå‹•æ…‹æ‹†è§£å­å•é¡Œã€å½™æ•´ç•°å¸¸ã€ä¸»å‹•è£œæŸ¥ã€å®¹éŒ¯ã€‚\n"
        "ä½ å¿…é ˆè¨˜ä½æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼ˆsession_historyï¼‰ï¼Œæ¯æ¬¡è¿½å•éƒ½è‡ªå‹•å¸¶å…¥å‰æ–‡æ‰€æœ‰ç•°å¸¸/å»ºè­°æ‘˜è¦ï¼Œ\n"
        "é‡åˆ°è¿½å•æ™‚ï¼Œè«‹æ¯”å°å‰æ¬¡èˆ‡æœ¬è¼ªæ‰€æœ‰ç•°å¸¸/å»ºè­°ï¼Œåªè£œå……æ–°ç™¼ç¾ï¼Œä¸é‡è¤‡ã€‚\n"
        "å¦‚æœ¬è¼ªç„¡æ–°ç•°å¸¸/å»ºè­°ï¼Œè«‹æ˜ç¢ºå›è¦†ã€æœ¬è¼ªå·²ç„¡æ–°ç•°å¸¸/å»ºè­°ã€ã€‚\n"
        "æ‰€æœ‰å›è¦†å¿…é ˆåˆ†ç¾¤æ¢åˆ—ï¼ˆä¸»å› /åš´é‡ç¨‹åº¦/å…·é«”å»ºè­°ï¼‰ï¼Œä¸å¯æœ‰å¤šé¤˜è´…è¿°æˆ–ç½é ­æ¨™é¡Œã€‚\n"
        + LLM_INTENT_DOC
    )
    prev_summary = ""
    if history:
        for msg in reversed(history):
            if msg.get("role") == "assistant" and msg.get("content"):
                prev_summary = msg["content"]
                break
    combined_summary = ""
    if prev_summary:
        combined_summary += "ã€å‰æ¬¡æ‘˜è¦ã€‘\n" + prev_summary + "\n"
    combined_summary += "ã€æœ¬è¼ªç•°å¸¸æ‘˜è¦ã€‘\n" + (current_summary if current_summary else "(ç„¡æ‘˜è¦)")

    final_prompt = (
        f"ã€ç”¨æˆ¶æœ¬è¼ªå•é¡Œã€‘\n{user_intent}\n"
        "è«‹æ ¹æ“šä¸‹åˆ— MCP å·¥å…·æŸ¥è©¢çµæœèˆ‡ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œ"
        "éˆæ´»é¸æ“‡æœ€é©åˆçš„å›è¦†çµæ§‹ï¼ˆå¦‚åˆ†ç¾¤æ¢åˆ—ã€ç¸½é«”å»ºè­°ã€è¶¨å‹¢åˆ†æç­‰ï¼‰ï¼Œ"
        "å‹™å¿…è²¼åˆç”¨æˆ¶æœ¬è¼ªå•é¡Œéœ€æ±‚ï¼Œä¸è¦æ­»æ¿å¥—ç”¨æ ¼å¼ã€‚"
        "å¦‚ç”¨æˆ¶å•ç¸½é«”å»ºè­°ï¼Œè«‹åªå›ç¸½é«”å»ºè­°ï¼›å¦‚å•ç•°å¸¸ä¸»å› ï¼Œè«‹åˆ†ç¾¤æ¢åˆ—ï¼›å¦‚å•è¶¨å‹¢ï¼Œè«‹æ‘˜è¦è¶¨å‹¢ã€‚"
        "å¦‚æœ¬è¼ªç„¡æ–°é‡é»ï¼Œè«‹æ˜ç¢ºå›è¦†ã€æœ¬è¼ªå·²ç„¡æ–°ç•°å¸¸/å»ºè­°ã€ã€‚\n"
        f"{combined_summary}"
    )
    messages_final = []
    if history:
        messages_final.extend(history)
    messages_final.append({"role": "system", "content": sys_prompt})
    messages_final.append({"role": "user", "content": final_prompt})
    llm_reply = call_llm(messages_final)
    step_outputs[5] = llm_reply

    # === æ–°å¢ï¼šç„¡è«–return_summaryç‚ºTrueæˆ–Falseï¼Œéƒ½å¯å›å‚³ä¸‰ä»¶äº‹ ===
    if return_summary:
        return step_outputs, llm_reply, current_summary
    else:
        return llm_reply

def run_agent(user_query, session_history=None, return_summary=False):
    result = run_agent_smart(user_query, session_history=session_history, return_summary=True)
    if return_summary:
        return result  # (step_outputs, reply, summary_section)
    else:
        return result[1]  # åªå›å‚³ reply

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å‘¼å« LLM ç”Ÿæˆå»ºè­°
def call_llm(messages, model="gpt-4o", api_key=None, temperature=0.0):
    api_key = api_key or OPENAI_API_KEY
    client = OpenAI(api_key=api_key)
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[LLMå‘¼å«å¤±æ•—] {str(e)}"

# ========= ä¸»è¦æµç¨‹ =========
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch", help="æ‰¹æ¬¡IDï¼Œå¤šå€‹ç”¨é€—è™Ÿåˆ†éš”ã€‚ä¸æŒ‡å®šå‰‡è‡ªå‹•æŠ“å…¨éƒ¨ã€‚")
    parser.add_argument("--extra", nargs="*", help="å…¶ä»–åƒæ•¸ï¼Œå¦‚ date=2024-05-28")
    parser.add_argument("--decompose", action="store_true", help="åƒ…é€²è¡Œèªæ„åˆ†é¡èˆ‡ tool_call æ‹†è§£")
    args = parser.parse_args()

    if args.decompose:
        query = input("è«‹è¼¸å…¥è¦åˆ†æçš„å•é¡Œï¼š")
        result = decompose_query(query)
        print("\nâœ… èªæ„åˆ†é¡ï¼š", result.get("intent"))
        print("ğŸ§© tool_calls æ‹†è§£ï¼š")
        for i, call in enumerate(result.get("tool_calls", []), 1):
            print(f"  {i}. {call}")
        exit(0)

    # å–å¾—æ‰€æœ‰æ‰¹æ¬¡ID
    if args.batch:
        batch_ids = [x.strip().zfill(2) for x in args.batch.split(",") if x.strip()]
    else:
        cache_dir = Path(JSON_CACHE)
        batch_ids = [f.stem for f in cache_dir.glob("*.json")]
        batch_ids.sort()

    # è™•ç†é¡å¤–åƒæ•¸
    extra_dict = {}
    if args.extra:
        for item in args.extra:
            if "=" in item:
                k, v = item.split("=", 1)
                extra_dict[k] = v

    # è®“ä½¿ç”¨è€…è¼¸å…¥è‡ªç„¶èªè¨€æŸ¥è©¢
    query = input("è«‹è¼¸å…¥æŸ¥è©¢éœ€æ±‚ï¼š")
    decomp = decompose_query(query)
    tool_calls = decomp.get("tool_calls", [])
    intent = decomp.get("intent", "")

    all_batch_summaries = []
    for batch_id in batch_ids:
        user_input_dict = {"batch_id": batch_id}
        user_input_dict.update(extra_dict)
        tool_results = {}
        for call in tool_calls:
            tool = call["tool"]
            args_dict = {arg: user_input_dict.get(arg, "") for arg in call["args"]}
            tool_result = call_server(tool, args_dict)
            tool_results[tool] = summarize_tool_result(tool, tool_result)
        all_batch_summaries.append(summarize_batch_context(batch_id, tool_results))

    # çµ„åˆæ‰€æœ‰æ‰¹æ¬¡çš„é‡é»æ‘˜è¦
    prompt = "ä¸‹æ–¹ç‚ºå¤šå€‹æ‰¹æ¬¡çš„é‡é»æ‘˜è¦ï¼š\n"
    for batch_summary in all_batch_summaries:
        prompt += batch_summary
    prompt += "\nè«‹ä¾æ“šä¸Šè¿°æ‰€æœ‰æ‰¹æ¬¡è³‡è¨Šï¼Œæ­¸ç´å„æ‰¹æ¬¡çš„å“è³ªå•é¡Œã€æ˜¯å¦éœ€ç¾å ´æ”¹å–„ï¼Œä¸¦æå‡ºç¸½é«”æª¢è¨/æ”¹å–„å»ºè­°ã€‚"

    system_msg = "ä½ æ˜¯ç”¢ç·šå°ˆå®¶ï¼Œè«‹æ ¹æ“šè³‡æ–™ç”¢ç”Ÿå°ˆæ¥­å»ºè­°ã€‚"
    print("=== å‚³é€çµ¦ LLM çš„ prompt ===\n", prompt)
    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": prompt}
    ]
    llm_reply = call_llm(messages)
    print("=== LLM å›è¦† ===\n", llm_reply)

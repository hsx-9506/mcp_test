import os
import re
import json
import requests
from openai import OpenAI
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, List
import argparse
import config.setting as setting
from config.prompts import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE

load_dotenv()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ èªæ„æ‹†è§£èˆ‡è‡ªå‹• tool_call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_json(text: str) -> str:
    """
    å¾ LLM å›è¦†ä¸­æ“·å– JSON å€å¡Šï¼Œè‹¥ç„¡å‰‡å›å‚³åŸæ–‡
    """
    match = re.search(r"```(?:json)?\\s*(\\{[\\s\\S]*?\\})\\s*```", text)
    if match:
        return match.group(1)
    return text

def decompose_query(user_input: str) -> dict:
    """
    èªæ„æ‹†è§£ä¸»æµç¨‹ï¼Œå›å‚³ LLM æ‹†è§£å¾Œçš„ intent/tool_calls çµæ§‹
    """
    user_prompt = USER_PROMPT_TEMPLATE.format(user_input=user_input)
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt}
    ]
    response_text = call_llm(messages)
    json_part = extract_json(response_text)
    try:
        result = json.loads(json_part)
    except json.JSONDecodeError:
        raise ValueError("âŒ ç„¡æ³•è§£æ LLM å›æ‡‰ï¼š", response_text)
    return result

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# çµ±ä¸€å‘¼å« unified_server æŸ¥è©¢
def call_server(tool, args):
    params = {"type": tool}
    params.update(args)
    try:
        resp = requests.get(setting.UNIFIED_SERVER_URL, params=params, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        return {"status": "ERROR", "data": str(e)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å½ˆæ€§æ‘˜è¦å„ç¨® tool çš„å›å‚³çµæœï¼ˆå…¨ tool æ”¯æ´ï¼‰
def summarize_tool_result(tool, tool_result):
    if tool_result.get("status") != "OK":
        return f"ã€ç„¡è³‡æ–™/ç•°å¸¸: {tool_result.get('data','')}ã€‘"
    data = tool_result.get("data", [])

    if tool == "batch_anomaly":
        if not data:
            return "æœ¬æ‰¹æ¬¡ç„¡ç•°å¸¸è³‡æ–™"
        item = data[0]
        result = [
            f"æ‰¹æ¬¡ï¼š{item.get('batch_id', '')}",
            f"ç”¢å“ï¼š{item.get('product_name', '')}",
            f"æ©Ÿå°ï¼š{item.get('machine_id', '')}",
            f"ç•°å¸¸æ•¸ï¼š{item.get('abnormal_count', 0)}"
        ]
        if item.get("abnormal_features"):
            result.append("ç•°å¸¸æ˜ç´°ï¼š")
            for f in item["abnormal_features"]:
                desc = f"- ç‰¹æ€§ï¼š{f.get('feature_name', '')} (Cpk={f.get('cpk', '')})"
                if f.get("cpk_alert"):
                    desc += f" [Cpkè­¦å‘Š: {f.get('cpk_reason', '')}]"
                if f.get("ppk_alert"):
                    desc += f" [Ppkè­¦å‘Š: {f.get('ppk_reason', '')}]"
                if f.get("abnormal_detail"):
                    desc += f" | æ˜ç´°: {','.join([str(x) for x in f['abnormal_detail']])}"
                result.append(desc)
        else:
            result.append("æœ¬æ‰¹æ¬¡ç„¡ç•°å¸¸ç‰¹æ€§")
        return "\n".join(result)

    if tool == "spc_summary":
        if not data:
            return "ç„¡ SPC è£½ç¨‹èƒ½åŠ›è³‡æ–™"
        item = data[0]
        lines = [
            f"æ‰¹æ¬¡ï¼š{item.get('batch_id', '')}",
            f"ç”¢å“ï¼š{item.get('product_name', '')}",
            f"æ©Ÿå°ï¼š{item.get('machine_id', '')}",
            f"SPCé …ç›®æ•¸ï¼š{item.get('total_spc_items', 0)}",
        ]
        if "spc_items" in item:
            for spc in item["spc_items"]:
                lines.append(f"- ç‰¹æ€§ï¼š{spc.get('feature_name', '')} | Cpk={spc.get('cpk', '')} | Ppk={spc.get('ppk', '')} | è­¦å‘Š: {'æ˜¯' if spc.get('cpk_alert') else 'å¦'}")
        else:
            lines.append("ç„¡ SPC æ˜ç´°")
        return "\n".join(lines)

    if tool == "production_summary":
        if not data:
            return "æœ¬æ—¥ç„¡ç”¢èƒ½è³‡æ–™"
        header = ["æ©Ÿå°", "ç”¢ç·š", "ç­åˆ¥", "ç›®æ¨™ç”¢é‡", "å¯¦éš›ç”¢é‡", "é”æˆç‡(%)"]
        rows = [
            [row.get("machine_id", ""), row.get("line", ""), row.get("shift", ""),
             row.get("target_qty", ""), row.get("actual_qty", ""), row.get("achieve_rate", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "downtime_summary":
        if not data:
            return "æœ¬æ—¥ç„¡åœæ©Ÿç´€éŒ„"
        header = ["æ©Ÿå°", "ç”¢ç·š", "ç­åˆ¥", "åœæ©Ÿæ¬¡æ•¸", "åœæ©Ÿç¸½æ™‚æ•¸(åˆ†é˜)", "ä¸»å› ", "å‚™è¨»"]
        rows = [
            [row.get("machine_id", ""), row.get("line", ""), row.get("shift", ""), row.get("event_count", ""),
             row.get("total_minutes", ""), row.get("main_reason", ""), row.get("remark", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "yield_summary":
        if not data:
            return "æœ¬æ—¥ç„¡è‰¯ç‡ç´€éŒ„"
        header = ["ç”¢å“", "ç”¢ç·š", "ç­åˆ¥", "è‰¯å“æ•¸", "ä¸è‰¯å“æ•¸", "è‰¯ç‡(%)"]
        rows = [
            [row.get("product", ""), row.get("line", ""), row.get("shift", ""),
             row.get("good_qty", ""), row.get("ng_qty", ""), row.get("yield_percent", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "anomaly_trend":
        if not data:
            return "å€é–“å…§ç„¡ç•°å¸¸äº‹ä»¶"
        header = ["æ—¥æœŸ", "æ©Ÿå°", "ç”¢ç·š", "ç•°å¸¸é¡å‹", "ç•°å¸¸ä»£ç¢¼", "æ¬¡æ•¸", "å‚™è¨»"]
        rows = [
            [row.get("date",""), row.get("machine_id", ""), row.get("line", ""), row.get("event_type", ""),
             row.get("abnormal_code", ""), row.get("count", ""), row.get("remark", "")]
            for row in data
        ]
        table = "\t".join(header) + "\n"
        for r in rows:
            table += "\t".join([str(x) for x in r]) + "\n"
        return table

    if tool == "KPI_summary":
        if not data:
            return "æœ¬æ—¥ç„¡KPIç´€éŒ„"
        header = list(data[0].keys())
        table = "\t".join(header) + "\n"
        for row in data:
            table += "\t".join([str(row.get(h, "")) for h in header]) + "\n"
        return table

    if tool == "issue_tracker":
        if not data:
            return "ç„¡æœªçµæ¡ˆå·¥å–®"
        header = list(data[0].keys())
        table = "\t".join(header) + "\n"
        for row in data:
            table += "\t".join([str(row.get(h, "")) for h in header]) + "\n"
        return table

    # fallback
    return json.dumps(data, ensure_ascii=False, indent=2) if data else "ç„¡è³‡æ–™"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å½™ç¸½æ‰¹æ¬¡çš„å„å·¥å…·æ‘˜è¦
def summarize_batch_context(batch_id, tool_results: Dict[str, str]):
    # åªæ“·å–ç•°å¸¸æ•¸èˆ‡SPCè­¦å‘Šæ•¸ç­‰é‡é»
    batch_anomaly = tool_results.get("batch_anomaly", "")
    spc_summary = tool_results.get("spc_summary", "")

    # æ“·å–ç•°å¸¸æ•¸
    abnormal_count = 0
    if "ç•°å¸¸æ•¸ï¼š" in batch_anomaly:
        try:
            abnormal_count = int(batch_anomaly.split("ç•°å¸¸æ•¸ï¼š")[1].split("\n")[0])
        except Exception:
            abnormal_count = 0

    # æ“·å–SPCè­¦å‘Šæ•¸
    spc_alert_count = 0
    if "è­¦å‘Š: æ˜¯" in spc_summary:
        spc_alert_count = spc_summary.count("è­¦å‘Š: æ˜¯")

    summary = (
        f"æ‰¹æ¬¡ï¼š{batch_id} | ç•°å¸¸æ•¸ï¼š{abnormal_count} | SPCè­¦å‘Šæ•¸ï¼š{spc_alert_count}\n"
    )
    return summary

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»æµç¨‹ï¼šå°‡æŸ¥è©¢è½‰æ›ç‚º LLM å›è¦†
def run_agent(query, batch_ids=None, extra_dict=None):
    """
    å°‡ä¸»æµç¨‹åŒ…è£æˆå‡½å¼ï¼Œä¾› UI æˆ–å…¶ä»–ç¨‹å¼ç›´æ¥å‘¼å«ã€‚
    :param query: ä½¿ç”¨è€…æŸ¥è©¢å­—ä¸²
    :param batch_ids: æ‰¹æ¬¡ID listï¼Œé è¨­è‡ªå‹•æŠ“å…¨éƒ¨
    :param extra_dict: å…¶ä»–æŸ¥è©¢åƒæ•¸ dict
    :return: LLM å›è¦†å­—ä¸²
    """
    if batch_ids is None:
        cache_dir = Path(setting.JSON_CACHE)
        batch_ids = [f.stem for f in cache_dir.glob("*.json")]
        batch_ids.sort()
    if extra_dict is None:
        extra_dict = {}
    decomp = decompose_query(query)
    tool_calls = decomp.get("tool_calls", [])
    all_batch_summaries = []
    for batch_id in batch_ids:
        user_input_dict = {"batch_id": batch_id}
        user_input_dict.update(extra_dict)
        tool_results = {}
        for call in tool_calls:
            tool = call["tool"]
            args_dict = {arg: user_input_dict.get(arg, "") for arg in call["args"]}
            tool_result = call_server(tool, args_dict)
            tool_results[tool] = summarize_tool_result(tool, tool_result)
        all_batch_summaries.append(summarize_batch_context(batch_id, tool_results))
    prompt = "ä¸‹æ–¹ç‚ºå¤šå€‹æ‰¹æ¬¡çš„é‡é»æ‘˜è¦ï¼š\n"
    for batch_summary in all_batch_summaries:
        prompt += batch_summary
    prompt += "\nè«‹ä¾æ“šä¸Šè¿°æ‰€æœ‰æ‰¹æ¬¡è³‡è¨Šï¼Œæ­¸ç´å„æ‰¹æ¬¡çš„å“è³ªå•é¡Œã€æ˜¯å¦éœ€ç¾å ´æ”¹å–„ï¼Œä¸¦æå‡ºç¸½é«”æª¢è¨/æ”¹å–„å»ºè­°ã€‚"
    prompt += "\nç›´æ¥ç”¨æ¸…å–®èˆ‡æ®µè½æ¢åˆ—å¼å›ç­”ï¼Œä¸è¦åŠ ä»»ä½•è£é£¾ç”¨ç¬¦è™Ÿã€‚"
    system_msg = "ä½ æ˜¯ç”¢ç·šå°ˆå®¶ï¼Œè«‹æ ¹æ“šè³‡æ–™ç”¢ç”Ÿå°ˆæ¥­å»ºè­°ã€‚"
    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": prompt}
    ]
    llm_reply = call_llm(messages)
    return llm_reply

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å‘¼å« LLM ç”Ÿæˆå»ºè­°
def call_llm(
    messages: list,
    model: str = "gpt-4o",
    api_key: str = None,
    temperature: float = 0.0
) -> str:
    """
    çµ±ä¸€çš„ OpenAI LLM å‘¼å«ä»‹é¢
    :param messages: [{"role": "system", "content": ...}, {"role": "user", "content": ...}]
    :param model: é è¨­ gpt-4o
    :param api_key: å¯é¸ï¼Œé è¨­ç”¨ OPENAI_API_KEY
    :param temperature: æº«åº¦åƒæ•¸
    :return: LLM å›è¦†æ–‡å­—
    """
    api_key = setting.OPENAI_API_KEY
    client = OpenAI(api_key=api_key)
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[LLMå‘¼å«å¤±æ•—] {str(e)}"

# ========= ä¸»è¦æµç¨‹ =========
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch", help="æ‰¹æ¬¡IDï¼Œå¤šå€‹ç”¨é€—è™Ÿåˆ†éš”ã€‚ä¸æŒ‡å®šå‰‡è‡ªå‹•æŠ“å…¨éƒ¨ã€‚")
    parser.add_argument("--extra", nargs="*", help="å…¶ä»–åƒæ•¸ï¼Œå¦‚ date=2024-05-28")
    parser.add_argument("--decompose", action="store_true", help="åƒ…é€²è¡Œèªæ„åˆ†é¡èˆ‡ tool_call æ‹†è§£")
    args = parser.parse_args()

    if args.decompose:
        query = input("è«‹è¼¸å…¥è¦åˆ†æçš„å•é¡Œï¼š")
        result = decompose_query(query)
        print("\nâœ… èªæ„åˆ†é¡ï¼š", result.get("intent"))
        print("ğŸ§© tool_calls æ‹†è§£ï¼š")
        for i, call in enumerate(result.get("tool_calls", []), 1):
            print(f"  {i}. {call}")
        exit(0)

    # å–å¾—æ‰€æœ‰æ‰¹æ¬¡ID
    if args.batch:
        batch_ids = [x.strip().zfill(2) for x in args.batch.split(",") if x.strip()]
    else:
        cache_dir = Path(setting.JSON_CACHE)
        batch_ids = [f.stem for f in cache_dir.glob("*.json")]
        batch_ids.sort()

    # è™•ç†é¡å¤–åƒæ•¸
    extra_dict = {}
    if args.extra:
        for item in args.extra:
            if "=" in item:
                k, v = item.split("=", 1)
                extra_dict[k] = v

    # è®“ä½¿ç”¨è€…è¼¸å…¥è‡ªç„¶èªè¨€æŸ¥è©¢
    query = input("è«‹è¼¸å…¥æŸ¥è©¢éœ€æ±‚ï¼š")
    decomp = decompose_query(query)
    tool_calls = decomp.get("tool_calls", [])
    intent = decomp.get("intent", "")

    all_batch_summaries = []
    for batch_id in batch_ids:
        user_input_dict = {"batch_id": batch_id}
        user_input_dict.update(extra_dict)
        tool_results = {}
        for call in tool_calls:
            tool = call["tool"]
            args_dict = {arg: user_input_dict.get(arg, "") for arg in call["args"]}
            tool_result = call_server(tool, args_dict)
            tool_results[tool] = summarize_tool_result(tool, tool_result)
        all_batch_summaries.append(summarize_batch_context(batch_id, tool_results))

    # çµ„åˆæ‰€æœ‰æ‰¹æ¬¡çš„é‡é»æ‘˜è¦
    prompt = "ä¸‹æ–¹ç‚ºå¤šå€‹æ‰¹æ¬¡çš„é‡é»æ‘˜è¦ï¼š\n"
    for batch_summary in all_batch_summaries:
        prompt += batch_summary
    prompt += "\nè«‹ä¾æ“šä¸Šè¿°æ‰€æœ‰æ‰¹æ¬¡è³‡è¨Šï¼Œæ­¸ç´å„æ‰¹æ¬¡çš„å“è³ªå•é¡Œã€æ˜¯å¦éœ€ç¾å ´æ”¹å–„ï¼Œä¸¦æå‡ºç¸½é«”æª¢è¨/æ”¹å–„å»ºè­°ã€‚"

    system_msg = "ä½ æ˜¯ç”¢ç·šå°ˆå®¶ï¼Œè«‹æ ¹æ“šè³‡æ–™ç”¢ç”Ÿå°ˆæ¥­å»ºè­°ã€‚"
    print("=== å‚³é€çµ¦ LLM çš„ prompt ===\n", prompt)
    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": prompt}
    ]
    llm_reply = call_llm(messages)
    print("=== LLM å›è¦† ===\n", llm_reply)
